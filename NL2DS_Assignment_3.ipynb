{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NL2DS-Assignment-3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShenshunYao/NLP-Course-in-McGill/blob/master/NL2DS_Assignment_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FuO82RTBftK"
      },
      "source": [
        "# Vector Space Model of Word Meaning\n",
        "## Shenshun Yao 260709204\n",
        "The goal of this assignment is to make you familiar with vector space model of word meaning. You may reuse some of functions you coded in Assignment 1. \n",
        "\n",
        "### Warning: This assignment may take substantial time to run if you are not optimizing your code. Make sure you have plenty of time to run if you are new to programming.\n",
        "\n",
        "Go to https://drive.google.com/drive/folders/1Pe6D713L9S0GWwb_XzeLXAeNZOrBqZaN?usp=sharing and click add shortcut to drive. This will add the data required for this problem set to your Google drive.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1LqHisiziX8Ri94Xs6Cv8mhx6vivFM3kS\" alt=\"Drawing\" height=\"300\"/>\n",
        "\n",
        "Caution: Since this is real language on Twitter and deals with current events, some of it could be disturbing. In the later section of the course, we will deal with ethics and what is appropriate and what is not. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UtZEcHthBeXz"
      },
      "source": [
        "Run the below code snippet. It will generate a URL which generates an authorization code.* Enter it below to give Colab access to your Google drive. \n",
        "\n",
        "*Copy function may not work. If so, manually copy the authorization code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KW-dce7oJlyr",
        "outputId": "e89ab117-88ca-4ba5-ecca-19cb1a65f73c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni2pYuuQKaHY"
      },
      "source": [
        "When you run the `ls` command below, you should see the files in the tweets folder.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYENtyc7SOxA",
        "outputId": "7883edfc-ad71-4b53-88c0-f7e9c479a889",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!ls \"/content/drive/My Drive/tweets\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "20000_tweets.jsonl\n",
            "20000_tweets.txt\n",
            "covid-tweets-2020-08-10-2020-08-21.tokenized.txt\n",
            "GoogleNews-vectors-negative300.bin.gz\n",
            "stop_words.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZMOmElPSPHk",
        "outputId": "cf143530-aeea-4fb8-9013-02842540c660",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# let's read tweets. These tweets are already tokenized and cleaned (Assignment 1)\n",
        "tweets = open(\"/content/drive/My Drive/tweets/covid-tweets-2020-08-10-2020-08-21.tokenized.txt\", \"r\").read().split(\"\\n\")\n",
        "tweets = [tweet.split() for tweet in tweets]\n",
        "\n",
        "print(len(tweets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "312878\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVkL6n6jU1LU"
      },
      "source": [
        "## Problem 1: Word space model\n",
        "\n",
        "Compute the most important context words of `ventilator`. Use Pointwise Mutual Information (PMI) to rank the context words (Refer to Lecture 4).\n",
        "\n",
        "We define context as up to 3 words to the left and 3 words to the right. Ignore stop words and words that do not start with [a-z#]. Also ignore words that are not in the top 1000 frequent words.\n",
        "\n",
        "These context words define the dimensions of the vector space model. Represent each word as a vector (dictionary/counter) of context words with PMI as the importance of the context word. Print the top 20 context words for each.\n",
        "\n",
        "This is the sample output I got for `ventilator`. Your numbers need not match mine but the ranked list should look close to what I have.\n",
        "\n",
        "```\n",
        "[('put', 18.280538283196606), ('wearing', 17.587373569812726), ('even', 17.58651933524197), ('like', 17.402738298715878), ('covid', 17.172590097063086), ('patients', 16.894419647496004), ('use', 16.894298589380956), ('die', 16.89426559608771), ('days', 16.89415252713107), ('needed', 16.489137134110106), ('month', 16.48907033839664), ('weeks', 16.488913820220848), ('away', 16.48879303327717), ('week', 16.488739054051933), ('person', 16.488678720881293), ('good', 16.488160838026904), ('deaths', 16.487822204799755), ('go', 16.487564042558112), ('would', 16.48707075078768), ('one', 16.48706217686235), ('get', 16.486565870239033)]\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0SbP-yfiaL3"
      },
      "source": [
        "Let's first load stop words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFoeDtZPTy-g",
        "outputId": "38f28002-b927-4adb-966d-705655d90463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "stop_words = set()\n",
        "def load_stop_words():\n",
        "  words = open(\"/content/drive/My Drive/tweets/stop_words.txt\", \"r\").read().split(\"\\n\")\n",
        "  for word in words:\n",
        "    stop_words.add(word.strip())\n",
        "\n",
        "load_stop_words()\n",
        "print(len(stop_words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "134\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owgu_aZnqvzW"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "pattern = re.compile(r\"^[a-z#].*\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rPrg52i836gJ"
      },
      "source": [
        "def clean_a_tweet(tweet,center):\n",
        "  cleaned_tweet = []\n",
        "  for word in tweet:\n",
        "    if word == center or (pattern.match(word) and word not in stop_words):\n",
        "      cleaned_tweet.append(word)\n",
        "  return cleaned_tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jd0mSpn97iol"
      },
      "source": [
        "def keep_topwords(tweet, center, topwords):\n",
        "  tweet_keepTopwords = []\n",
        "  for word in tweet:\n",
        "    if word == center or word in topwords:\n",
        "      tweet_keepTopwords.append(word)\n",
        "  return tweet_keepTopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkoSUGYOYPVi"
      },
      "source": [
        "def buildContext(tweet,center,pre,post):\n",
        "  for i,w in enumerate(tweet):\n",
        "    if w == center:\n",
        "      vector = [tweet[i + k] for k in range(-pre, post+1) if 0 <= (i + k) < len(tweet)]\n",
        "      if vector != []:\n",
        "        return vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJF6PXSHQBiR"
      },
      "source": [
        "Let's build the word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3ZF6XatF1Bp"
      },
      "source": [
        "from itertools import combinations\n",
        "def countUnigram(tweets):\n",
        "  res = Counter()\n",
        "  for tweet in tweets:\n",
        "    for x in tweet:\n",
        "      res[x] += 1\n",
        "  return res\n",
        "\n",
        "def countBigram(tweets):\n",
        "  res = Counter()\n",
        "  for tweet in tweets:\n",
        "    for x, y in map(sorted, combinations(tweet, 2)):\n",
        "        res[(x, y)] += 1\n",
        "  return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCsiynz3XouH"
      },
      "source": [
        "cx = countUnigram(tweets)\n",
        "cxy = countBigram(tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgzjPvFxXvUy"
      },
      "source": [
        "N = sum(cx.values())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COCn379_Uuyx"
      },
      "source": [
        "from math import log2\n",
        "PMI_dict = Counter()\n",
        "for (x, y), n in cxy.items():\n",
        "    ppmi = log2((n / N) / (cx[x] / N) / (cx[y] / N))\n",
        "    PMI_dict[(x,y)] = ppmi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ896-ClQBKM",
        "outputId": "a5ed35c4-f57b-4abd-d0a2-c10fb3616b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from math import log2\n",
        "\n",
        "def PMI(word1, word2):\n",
        "  # You have to store frequencies of individual words and (word, context word) \n",
        "  # pairs to compute this. You can compute them beforehand in order to avoid \n",
        "  # counting every time when this function is called.\n",
        "  # Write your code\n",
        "  if (word1,word2) in PMI_dict.keys():\n",
        "    return PMI_dict[(word1,word2)]  \n",
        "  elif (word2,word1) in PMI_dict.keys():\n",
        "    return PMI_dict[(word2,word1)]\n",
        "\n",
        "  \n",
        "def build_word_vector(word):\n",
        "  # Write your code\n",
        "  contexts = []\n",
        "  for tweet in tweets:\n",
        "    context = buildContext(tweet,word,3,3)\n",
        "    if context is not None:\n",
        "      contexts.append(context)\n",
        "  cleaned_tweets = [clean_a_tweet(tweet,word) for tweet in tweets]\n",
        "  cleaned_contexts = [clean_a_tweet(context,word) for context in contexts]\n",
        "  topwords = list(map(lambda x: x[0], Counter(x for tweet in cleaned_tweets for x in set(tweet)).most_common(1000)))\n",
        "  final_contexts = [keep_topwords(context,word,topwords) for context in cleaned_contexts]\n",
        "  dict = {}\n",
        "  for context in final_contexts:\n",
        "    for element in context:\n",
        "      if element != word:\n",
        "        pmi = PMI(word, element)\n",
        "        dict[element] = pmi\n",
        "  return dict\n",
        "\n",
        "def print_top_dimensions(word_vector, n):\n",
        "  # print top n dimensions sorted in the order of importance.\n",
        "  print (Counter(word_vector).most_common(n))\n",
        "\n",
        "v1 = build_word_vector('ventilator')\n",
        "print_top_dimensions(v1, 20) # print top 20 dimensions along with their weights\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('patient', 10.015404592729878), ('hospital', 8.658607886302383), ('patients', 8.347521351062447), ('k', 7.8989751013209855), ('put', 7.78973786526773), ('enjoy', 7.748305367528045), ('week', 7.688700922845756), ('wearing', 7.656846138567409), ('top', 7.626137162307978), ('brain', 7.575049274693839), ('end', 7.462851842808969), ('weeks', 7.432231951047436), ('died', 7.338278971032686), ('increase', 7.223404257292791), ('die', 7.219919118958893), ('spent', 7.216329148436841), ('days', 7.180218496199567), ('tried', 7.143470720095273), ('month', 7.11959162455242), ('ok', 7.115714451993435)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLZlAXuUVQXj"
      },
      "source": [
        "## Problem 2: Compute the word similarity between words using Cosine Similarity.\n",
        "\n",
        "Compute cosine similarity between the following pair of words: \n",
        "```\n",
        "('ventilator', 'covid-19')\n",
        "('ventilator', 'lockdown')\n",
        "('ventilator', 'mask')\n",
        "('ventilator', 'ppe')\n",
        "```\n",
        "\n",
        "Outputs of my code are:\n",
        "```\n",
        "('ventilator', 'covid-19') 0.17076006036635358\n",
        "('ventilator', 'lockdown') ...\n",
        "('ventilator', 'mask') 0.19229601085517933\n",
        "('ventilator', 'ppe') ...\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4x3CVEnsctn",
        "outputId": "e8b47afd-21f7-4f90-e799-ab01d915c16d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "import numpy as np\n",
        "def cosine_similarity(vector1, vector2):\n",
        "  # write your code\n",
        "  v1 = list(vector1.values())\n",
        "  v2 = list(vector2.values())\n",
        "  if len(v1) < len(v2):\n",
        "    v1.extend([0] * (len(v2) - len(v1)))\n",
        "  elif len(v2) < len(v1):\n",
        "    v2.extend([0] * (len(v1) - len(v2)))\n",
        "  a = np.array(v1)\n",
        "  b = np.array(v2)\n",
        "  dot = np.dot(a, b)\n",
        "  norma = np.linalg.norm(a)\n",
        "  normb = np.linalg.norm(b)\n",
        "  cos = dot / (norma * normb) \n",
        "  return cos\n",
        "\n",
        "ventilator = build_word_vector('ventilator')\n",
        "covid19 = build_word_vector('covid-19')\n",
        "lockdown = build_word_vector('lockdown')\n",
        "mask = build_word_vector('mask')\n",
        "ppe = build_word_vector('ppe')\n",
        "\n",
        "print(('ventilator', 'covid-19'), cosine_similarity(ventilator, covid19))\n",
        "print(('ventilator', 'lockdown'), cosine_similarity(ventilator, lockdown))\n",
        "print(('ventilator', 'mask'), cosine_similarity(ventilator, mask))\n",
        "print(('ventilator', 'ppe'), cosine_similarity(ventilator, ppe))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('ventilator', 'covid-19') 0.3454885872592941\n",
            "('ventilator', 'lockdown') 0.3501982721499012\n",
            "('ventilator', 'mask') 0.37104601269173726\n",
            "('ventilator', 'ppe') 0.5622871601587083\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDJHApLrmLnw"
      },
      "source": [
        "## Problem 3: What can you tell about these words from the similarities?\n",
        "\n",
        "1. `ventilator` when compared with `covid-19, lockdown, mask, ppe`\n",
        "2. `pandemic` when compared with `covid-19, lockdown, mask, ppe`\n",
        "3. `president` compared with `trump, biden`\n",
        "4. `trudeau` compared with `trump, biden`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sieceoEN_WmQ"
      },
      "source": [
        "def compare_similarity(word_a, word_list):\n",
        "  word_vector_a = build_word_vector(word_a)\n",
        "  for word_b in word_list:\n",
        "    word_vector_b = build_word_vector(word_b)\n",
        "    print((word_a, word_b), cosine_similarity(word_vector_a, word_vector_b))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkPh7GIk_a2n",
        "outputId": "bfff3301-d5e7-4e79-8a01-8fb262058545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "compare_similarity('ventilator', ['covid-19', 'lockdown', 'mask', 'ppe'])\n",
        "compare_similarity('pandemic', ['covid-19', 'lockdown', 'mask', 'ppe'])\n",
        "compare_similarity('president', ['trump', 'biden'])\n",
        "compare_similarity('trudeau', ['trump', 'biden'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('ventilator', 'covid-19') 0.3454885872592941\n",
            "('ventilator', 'lockdown') 0.3501982721499012\n",
            "('ventilator', 'mask') 0.37104601269173726\n",
            "('ventilator', 'ppe') 0.5622871601587083\n",
            "('pandemic', 'covid-19') 0.9802455806190478\n",
            "('pandemic', 'lockdown') 0.9651174342597776\n",
            "('pandemic', 'mask') 0.9542809248387079\n",
            "('pandemic', 'ppe') 0.6031245551902967\n",
            "('president', 'trump') 0.908315004755399\n",
            "('president', 'biden') 0.9357806765987713\n",
            "('trudeau', 'trump') 0.4850811155587289\n",
            "('trudeau', 'biden') 0.5292655869804244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sedQqvzGJEWX"
      },
      "source": [
        "After computing the cosine similarity between these four group of words. We found that in the first group, the similarites between these words are not high since they have different semantic meaning in the tweets. On the other hand, the words in the second group mostly have high similarities with each other which means that they are highly relevant in some liguistic senses. Moreover, we know that both 'trump' and 'biden' are highly relevant to 'president'. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Q-FpH2k7akG"
      },
      "source": [
        "# Let's play with word2vec\n",
        "\n",
        "First let's load word2vec. I am using [gensim](https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html) but feel free to use any libraries or tools."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11Jcaf1T9mHU"
      },
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "EMBEDDING_FILE = '/content/drive/My Drive/tweets/GoogleNews-vectors-negative300.bin.gz' # from above\n",
        "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, binary=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXHjFq6upAUA"
      },
      "source": [
        "## Problem 4: Compute the top 5 similar words of `ventilator` using word2vec?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xvff7yDBFUMg",
        "outputId": "6373c049-fa86-4e70-b37e-cbf4ba852b36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Write your code here\n",
        "print(word2vec.most_similar(positive=['ventilator'],topn=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('respirator', 0.7864563465118408), ('mechanical_ventilator', 0.7063840627670288), ('intensive_care', 0.6809945702552795), ('ventilators', 0.6683582067489624), ('breathing_tube', 0.6665509343147278)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufUX14RV2bCY"
      },
      "source": [
        "# Problem 5: Word analogy\n",
        "\n",
        "If I told you the plural of `car` is `cars`, can you automatically find the plural of `hyothesis` and `man` using word2vec.\n",
        "\n",
        "Similarly, if I told you a newborn `dog` is called `puppy`, can you automatically find what are the newborn words of `cat` and `sheep` using word2vec?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-E0KIkqOqD8H",
        "outputId": "8a3c2e92-f5c0-489e-9e7f-6f3f339c12d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "source": [
        "# Write your code here.\n",
        "print(word2vec.most_similar(positive=['hypothesis','cars'],negative=['car'],topn=5))\n",
        "print(word2vec.most_similar(positive=['man','cars'],negative=['car'],topn=5))\n",
        "print(word2vec.most_similar(positive=['cat','puppy'],negative=['dog'],topn=5))\n",
        "print(word2vec.most_similar(positive=['sheep','puppy'],negative=['dog'],topn=5))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[('hypotheses', 0.6453900337219238), ('theory', 0.5974409580230713), ('theories', 0.566754162311554), ('postulate', 0.5027041435241699), ('supposition', 0.500673770904541)]\n",
            "[('men', 0.5470781326293945), ('woman', 0.5195947885513306), ('boy', 0.48992404341697693), ('teenager', 0.4598117470741272), ('guy', 0.44635331630706787)]\n",
            "[('kitten', 0.7634989619255066), ('puppies', 0.7110899090766907), ('pup', 0.6929494142532349), ('kittens', 0.6888390779495239), ('cats', 0.6796488761901855)]\n",
            "[('lambs', 0.7109252214431763), ('ewes', 0.6476837396621704), ('cattle', 0.6313712000846863), ('cows', 0.6276622414588928), ('calves', 0.6254477500915527)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}